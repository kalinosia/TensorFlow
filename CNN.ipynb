{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "WIxqiSzYLE1d",
        "vJqyZTfxLOGZ"
      ],
      "authorship_tag": "ABX9TyMTC7OLq3xoqBPu6aaO1W0C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalinosia/TensorFlow/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n",
        "\n",
        "\n",
        "Let's start by importing required packages:\n",
        "\n",
        "*   os ‚Äî to read files and directory structure\n",
        "*   numpy ‚Äî for some matrix math outside of TensorFlow\n",
        "*   matplotlib.pyplot ‚Äî to plot the graph and display images in our training and validation data\n",
        "\n",
        "(If) we'll need [TensorFlow Datasets](https://www.tensorflow.org/datasets/), an API that simplifies downloading and accessing datasets, and provides several sample datasets to work with."
      ],
      "metadata": {
        "id": "wQz1COGPIi1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Import TensorFlow Datasets\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# Helper libraries\n",
        "import os  # os package is used to read files and directory structure\n",
        "import math\n",
        "import numpy as np  # numpy is used to convert python list to numpy array and to perform required matrix operations\n",
        "import matplotlib.pyplot as plt  #  matplotlib.pyplot is used to plot the graph and display images in our training and validation data.\n",
        "\n",
        "import glob  # flowers\n",
        "import shutil  # flowers\n",
        "\n",
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "m5qwO775IwwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification with Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "VMoQ8PtiIits"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "hEUBhjcKIimo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grayscale images in 10 categories üß• üë¢ üëó üëï üë° üéí üëö üëí \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This guide uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 $\\times$ 28 pixels), as seen here:\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\" width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>T-shirt/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Trouser</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Pullover</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Dress</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Coat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Sandal</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Shirt</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Sneaker</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Bag</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Ankle boot</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "ceFvtRtCJXRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ],
      "metadata": {
        "id": "OMzhkHivJWyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']"
      ],
      "metadata": {
        "id": "1RBMvvUcIltR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cats and dogs (Color images in 2 categories üêà üêï)\n",
        "The dataset we are using is a filtered version of <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\" target=\"_blank\">Dogs vs. Cats</a> dataset from Kaggle (ultimately, this dataset is provided by Microsoft Research).\n",
        "\n",
        "In previous Colabs, we've used <a href=\"https://www.tensorflow.org/datasets\" target=\"_blank\">TensorFlow Datasets</a>, which is a very easy and convenient way to use datasets. In this Colab however, we will make use of the class `tf.keras.preprocessing.image.ImageDataGenerator` which will read data from disk. We therefore need to directly download *Dogs vs. Cats* from a URL and unzip it to the Colab filesystem.\n",
        "\n",
        "The dataset we have downloaded has the following directory structure.\n",
        "\n",
        "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
        "<b>cats_and_dogs_filtered</b>\n",
        "|__ <b>train</b>\n",
        "    |______ <b>cats</b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ...]\n",
        "    |______ <b>dogs</b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n",
        "|__ <b>validation</b>\n",
        "    |______ <b>cats</b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ...]\n",
        "    |______ <b>dogs</b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]\n",
        "</pre>\n"
      ],
      "metadata": {
        "id": "38W5r1vVIiN9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHy9gd73H5Ns"
      },
      "outputs": [],
      "source": [
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "zip_dir = tf.keras.utils.get_file('cats_and_dogs_filterted.zip', origin=_URL, extract=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can list the directories with the following terminal command:\n",
        "zip_dir_base = os.path.dirname(zip_dir)\n",
        "!find $zip_dir_base -type d -print"
      ],
      "metadata": {
        "id": "wU-eJTWFKEoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll now assign variables with the proper file path for the training and validation sets.\n",
        "base_dir = os.path.join(os.path.dirname(zip_dir), 'cats_and_dogs_filtered')\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures"
      ],
      "metadata": {
        "id": "DqXbTiwWKE2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flowers (color images in 5 categories) üíê üåπ üåª üå∑ \n",
        "\n",
        "In order to build our image classifier, we can begin by downloading the flowers dataset. We first need to download the archive version of the dataset and after the download we are storing it to \"/tmp/\" directory.\n",
        "\n",
        "After downloading the dataset, we need to extract its contents.\n",
        "\n",
        "The dataset we downloaded contains images of 5 types of flowers:\n",
        "\n",
        "1. Rose\n",
        "2. Daisy\n",
        "3. Dandelion\n",
        "4. Sunflowers\n",
        "5. Tulips\n",
        "\n",
        "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
        "<b>flower_photos</b>\n",
        "|__ <b>daisy</b>\n",
        "|__ <b>dandelion</b>\n",
        "|__ <b>roses</b>\n",
        "|__ <b>sunflowers</b>\n",
        "|__ <b>tulips</b>\n",
        "</pre>"
      ],
      "metadata": {
        "id": "Gz0lRHGtBjG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "\n",
        "zip_file = tf.keras.utils.get_file(origin=_URL,\n",
        "                                   fname=\"flower_photos.tgz\",\n",
        "                                   extract=True)\n",
        "\n",
        "base_dir = os.path.join(os.path.dirname(zip_file), 'flower_photos')"
      ],
      "metadata": {
        "id": "rs-jSMN4CO9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['roses', 'daisy', 'dandelion', 'sunflowers', 'tulips']"
      ],
      "metadata": {
        "id": "KsxKYs9bCXle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore the data & Understanding our data"
      ],
      "metadata": {
        "id": "z676_MsMPqy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clothing üß•"
      ],
      "metadata": {
        "id": "svxhLuC_P4I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_examples = metadata.splits['train'].num_examples\n",
        "num_test_examples = metadata.splits['test'].num_examples\n",
        "print(\"Number of training examples: {}\".format(num_train_examples))\n",
        "print(\"Number of test examples:     {}\".format(num_test_examples))"
      ],
      "metadata": {
        "id": "nBmkgKmbPu5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cats and dogs üêï"
      ],
      "metadata": {
        "id": "Ay6HnjpFP3uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cats_tr = len(os.listdir(train_cats_dir))\n",
        "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
        "\n",
        "num_cats_val = len(os.listdir(validation_cats_dir))\n",
        "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
        "\n",
        "total_train = num_cats_tr + num_dogs_tr\n",
        "total_val = num_cats_val + num_dogs_val\n",
        "\n",
        "print('total training cat images:', num_cats_tr)\n",
        "print('total training dog images:', num_dogs_tr)\n",
        "\n",
        "print('total validation cat images:', num_cats_val)\n",
        "print('total validation dog images:', num_dogs_val)\n",
        "print(\"--\")\n",
        "print(\"Total training images:\", total_train)\n",
        "print(\"Total validation images:\", total_val)"
      ],
      "metadata": {
        "id": "IbhZq1nLP7SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting"
      ],
      "metadata": {
        "id": "-Put2zzkQCC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clothing üß•"
      ],
      "metadata": {
        "id": "z5d2hqW1QYpN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image Classification Without Image Augmentation"
      ],
      "metadata": {
        "id": "5oQ3_bMbDnvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the data\n",
        "\n",
        "The value of each pixel in the image data is an integer in the range [0,255]. For the model to work properly, these values need to be normalized to the range [0,1]. So here we create a normalization function, and then apply it to each image in the test and train datasets.\n"
      ],
      "metadata": {
        "id": "ZEsTsjZ9QgAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def normalize(images, labels):\n",
        "  images = tf.cast(images, tf.float32)\n",
        "  images /= 255\n",
        "  return images, labels\n",
        "\n",
        "# The map function applies the normalize function to each element in the train\n",
        "# and test datasets\n",
        "train_dataset =  train_dataset.map(normalize)\n",
        "test_dataset  =  test_dataset.map(normalize)\n",
        "\n",
        "# The first time you use the dataset, the images will be loaded from disk\n",
        "# Caching will keep them in memory, making training faster\n",
        "train_dataset =  train_dataset.cache()\n",
        "test_dataset  =  test_dataset.cache()\n"
      ],
      "metadata": {
        "id": "olJ_DvmTQaNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a single image, and remove the color dimension by reshaping\n",
        "for image, label in test_dataset.take(1):\n",
        "  break\n",
        "image = image.numpy().reshape((28,28))"
      ],
      "metadata": {
        "id": "a8AQkaxdQlpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cats and dogs üêï"
      ],
      "metadata": {
        "id": "-hhDwSRUQFYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 100  # Number of training examples to process before updating our models variables\n",
        "IMG_SHAPE  = 150  # Our training data consists of images with width of 150 pixels and height of 150 pixels"
      ],
      "metadata": {
        "id": "pquVWsu3QIgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Images must be formatted into appropriately pre-processed floating point tensors before being fed into the network. The steps involved in preparing these images are:\n",
        "\n",
        "1. Read images from the disk\n",
        "2. Decode contents of these images and convert it into proper grid format as per their RGB content\n",
        "3. Convert them into floating point tensors\n",
        "4. Rescale the tensors from values between 0 and 255 to values between 0 and 1, as neural networks prefer to deal with small input values.\n",
        "\n",
        "Fortunately, all these tasks can be done using the class **tf.keras.preprocessing.image.ImageDataGenerator**.\n",
        "\n",
        "We can set this up in a couple of lines of code."
      ],
      "metadata": {
        "id": "ThMyj1XzQrNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_generator      = ImageDataGenerator(rescale=1./255)  # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255)  # Generator for our validation data"
      ],
      "metadata": {
        "id": "wCltXsBzQu7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After defining our generators for training and validation images, **flow_from_directory** method will load images from the disk, apply rescaling, and resize them using single line of code."
      ],
      "metadata": {
        "id": "LjX-mBudQ0JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150)\n",
        "                                                           class_mode='binary')  # two clases\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              shuffle=False,\n",
        "                                                              target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150)\n",
        "                                                              class_mode='binary')"
      ],
      "metadata": {
        "id": "MSfDhwg5Q0wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image Classification With Image Augmentation\n",
        "\n",
        "Overfitting often occurs when we have a small number of training examples. One way to fix this problem is to augment our dataset so that it has sufficient number and variety of training examples. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples through random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This exposes the model to more aspects of the data, allowing it to generalize better.\n",
        "\n",
        "In tf.keras we can implement this using the same ImageDataGenerator class we used before. We can simply pass different transformations we would want to our dataset as a form of arguments and it will take care of applying it to the dataset during our training process.\n",
        "\n",
        "To start off, let's define a function that can display an image, so we can see the type of augmentation that has been performed. Then, we'll look at specific augmentations that we'll use during training.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "B_cTknV4y5sY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Flipping the image horizontally**\n",
        "\n",
        "We can begin by randomly applying horizontal flip augmentation to our dataset and seeing how individual images will look after the transformation. This is achieved by passing `horizontal_flip=True` as an argument to the `ImageDataGenerator` class.\n",
        "\n",
        "\n",
        "```\n",
        "image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
        "\n",
        "train_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE,\n",
        "                                               directory=train_dir,  # need dir!!\n",
        "                                               shuffle=True,\n",
        "                                               target_size=(IMG_SHAPE,IMG_SHAPE))\n",
        "```\n",
        "See:\n",
        "```\n",
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
        "plotImages(augmented_images)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HnQxVZNRzTqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rotating the image**\n",
        "\n",
        "The rotation augmentation will randomly rotate the image up to a specified number of degrees. Here, we'll set it to 45.\n",
        "\n",
        "```\n",
        "image_gen = ImageDataGenerator(rescale=1./255, rotation_range=45)\n",
        "\n",
        "train_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE,\n",
        "                                               directory=train_dir,\n",
        "                                               shuffle=True,\n",
        "                                               target_size=(IMG_SHAPE,IMG_SHAPE))\n",
        "```"
      ],
      "metadata": {
        "id": "VSy3kGppzjZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying Zoom**\n",
        "\n",
        "We can also apply Zoom augmentation to our dataset, zooming images up to 50% randomly.\n",
        "\n",
        "```\n",
        "image_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.5)\n",
        "\n",
        "train_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE,\n",
        "                                               directory=train_dir,\n",
        "                                               shuffle=True,\n",
        "                                               target_size=(IMG_SHAPE, IMG_SHAPE))\n",
        "```"
      ],
      "metadata": {
        "id": "r4oFV_3GzgqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Putting it all together"
      ],
      "metadata": {
        "id": "VTJp-fye0DU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_gen_train = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "train_data_gen = image_gen_train.flow_from_directory(batch_size=BATCH_SIZE,\n",
        "                                                     directory=train_dir,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_SHAPE,IMG_SHAPE),\n",
        "                                                     class_mode='binary')"
      ],
      "metadata": {
        "id": "1cYysjj3z_rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
        "plotImages(augmented_images)"
      ],
      "metadata": {
        "id": "XURgOY1S0MH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Validation Data generator**\n",
        "\n",
        "Generally, we only apply data augmentation to our training examples, since the original images should be representative of what our model needs to manage. So, in this case we are only rescaling our validation images and converting them into batches using ImageDataGenerator.\n"
      ],
      "metadata": {
        "id": "81vaSLYP0P5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_data_gen = image_gen_val.flow_from_directory(batch_size=BATCH_SIZE,\n",
        "                                                 directory=validation_dir,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='binary')"
      ],
      "metadata": {
        "id": "ya4vATZP0TMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flowers üå∑"
      ],
      "metadata": {
        "id": "2Oqb6b07CguF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create folders\n",
        "\n",
        "The code below creates a `train` and a `val` folder each containing 5 folders (one for each type of flower). It then moves the images from the original folders to these new folders such that 80% of the images go to the training set and 20% of the images go into the validation set. In the end our directory will have the following structure:\n",
        "\n",
        "\n",
        "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
        "<b>flower_photos</b>\n",
        "|__ <b>daisy</b>\n",
        "|__ <b>dandelion</b>\n",
        "|__ <b>roses</b>\n",
        "|__ <b>sunflowers</b>\n",
        "|__ <b>tulips</b>\n",
        "|__ <b>train</b>\n",
        "    |______ <b>daisy</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
        "    |______ <b>dandelion</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
        "    |______ <b>roses</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
        "    |______ <b>sunflowers</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
        "    |______ <b>tulips</b>: [1.jpg, 2.jpg, 3.jpg ....]\n",
        " |__ <b>val</b>\n",
        "    |______ <b>daisy</b>: [507.jpg, 508.jpg, 509.jpg ....]\n",
        "    |______ <b>dandelion</b>: [719.jpg, 720.jpg, 721.jpg ....]\n",
        "    |______ <b>roses</b>: [514.jpg, 515.jpg, 516.jpg ....]\n",
        "    |______ <b>sunflowers</b>: [560.jpg, 561.jpg, 562.jpg .....]\n",
        "    |______ <b>tulips</b>: [640.jpg, 641.jpg, 642.jpg ....]\n",
        "</pre>"
      ],
      "metadata": {
        "id": "BCq3x82CCpZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for cl in classes:\n",
        "  img_path = os.path.join(base_dir, cl)\n",
        "  images = glob.glob(img_path + '/*.jpg')\n",
        "  print(\"{}: {} Images\".format(cl, len(images)))\n",
        "  train, val = images[:round(len(images)*0.8)], images[round(len(images)*0.8):]\n",
        "\n",
        "  for t in train:\n",
        "    if not os.path.exists(os.path.join(base_dir, 'train', cl)):\n",
        "      os.makedirs(os.path.join(base_dir, 'train', cl))\n",
        "    shutil.move(t, os.path.join(base_dir, 'train', cl))\n",
        "\n",
        "  for v in val:\n",
        "    if not os.path.exists(os.path.join(base_dir, 'val', cl)):\n",
        "      os.makedirs(os.path.join(base_dir, 'val', cl))\n",
        "    shutil.move(v, os.path.join(base_dir, 'val', cl))"
      ],
      "metadata": {
        "id": "fKAamGIFCt7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')"
      ],
      "metadata": {
        "id": "3SEHDeDHEe2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Augmentation\n",
        "\n",
        "In the cell below, use ImageDataGenerator to create a transformation that rescales the images by 255 and that applies:\n",
        "\n",
        "- random 45 degree rotation\n",
        "- random zoom of up to 50%\n",
        "- random horizontal flip\n",
        "- width shift of 0.15\n",
        "- height shift of 0.15\n",
        "\n",
        "Then use the `.flow_from_directory` method to apply the above transformation to the images in our training set. Make sure you indicate the batch size, the path to the directory of the training images, the target size for the images, to shuffle the images, and to set the class mode to `sparse`.\n",
        "\n"
      ],
      "metadata": {
        "id": "GBV8_SetEjtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "IMG_SHAPE = 150"
      ],
      "metadata": {
        "id": "nH9VTOnHEujX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_gen_train = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      zoom_range=0.5,\n",
        "      horizontal_flip=True,\n",
        "      width_shift_range=0.15,\n",
        "      height_shift_range=0.15,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,\n",
        "                                                     directory=train_dir,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_SHAPE,IMG_SHAPE),\n",
        "                                                     class_mode='sparse')"
      ],
      "metadata": {
        "id": "PtmYBM1yGP7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally, we only apply data augmentation to our training examples. So, in the cell below, use ImageDataGenerator to create a transformation that only rescales the images by 255. \n",
        "Make sure you indicate the batch size, the path to the directory of the validation images, the target size for the images, and to set the class mode to sparse. Remember that it is not necessary to shuffle the images in the validation set. "
      ],
      "metadata": {
        "id": "HmqngIRuNLip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n",
        "                                                 directory=val_dir,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='sparse')"
      ],
      "metadata": {
        "id": "DPbEDAPFNKZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing"
      ],
      "metadata": {
        "id": "AUL_37BkQ8ZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clothing üß•"
      ],
      "metadata": {
        "id": "_l_GsVVHRAZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the image - voila a piece of fashion clothing\n",
        "plt.figure()\n",
        "plt.imshow(image, cmap=plt.cm.binary)\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# Display the first 25 images from the *training set* and display the class name below each image. Verify that the data is in the correct format and we're ready to build and train the network.\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "i = 0\n",
        "for (image, label) in test_dataset.take(25):\n",
        "    image = image.numpy().reshape((28,28))\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(image, cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[label])\n",
        "    i += 1\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hgfptvbMRAyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cats and dogs üêï"
      ],
      "metadata": {
        "id": "jVlDYO4TRBGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize our training images by getting a batch of images from the training generator, and then plotting a few of them using `matplotlib`.\n",
        "\n",
        "The `next` function returns a batch from the dataset. One batch is a tuple of (*many images*, *many labels*). For right now, we're discarding the labels because we just want to look at the images."
      ],
      "metadata": {
        "id": "1pGFweAHRI8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_training_images, _ = next(train_data_gen) \n",
        "\n",
        "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
        "def plotImages(images_arr):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip(images_arr, axes):\n",
        "        ax.imshow(img)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plotImages(sample_training_images[:5])  # Plot images 0-4"
      ],
      "metadata": {
        "id": "sEJC8D-9RDHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flowers üå∑"
      ],
      "metadata": {
        "id": "z1P77hy0MlGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize how a single image would look like 5 different times, when we pass these augmentations randomly to our dataset.\n",
        "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
        "plotImages(augmented_images)"
      ],
      "metadata": {
        "id": "hgwCr2MdMp6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "zjoSYKTORucB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clothing üß•\n",
        "\n",
        "The basic building block of a neural network is the *layer*. A layer extracts a representation from the data fed into it. Hopefully, a series of connected layers results in a representation that is meaningful for the problem at hand.\n",
        "\n",
        "Much of deep learning consists of chaining together simple layers. Most layers, like `tf.keras.layers.Dense`, have internal parameters which are adjusted (\"learned\") during training.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "This network layers are:\n",
        "\n",
        "* **\"convolutions\"** `tf.keras.layers.Conv2D and MaxPooling2D`‚Äî Network start with two pairs of Conv/MaxPool. The first layer is a Conv2D filters (3,3) being applied to the input image, retaining the original image size by using padding, and creating 32 output (convoluted) images (so this layer creates 32 convoluted images of the same size as input). After that, the 32 outputs are reduced in size using a MaxPooling2D (2,2) with a stride of 2. The next Conv2D also has a (3,3) kernel, takes the 32 images as input and creates 64 outputs which are again reduced in size by a MaxPooling2D layer. So far in the course, we have described what a Convolution does, but we haven't yet covered how you chain multiples of these together. We will get back to this in lesson 4 when we use color images. At this point, it's enough if you understand the kind of operation a convolutional filter performs\n",
        "\n",
        "* **output** `tf.keras.layers.Dense` ‚Äî A 128-neuron, followed by 10-node *softmax* layer. Each node represents a class of clothing. As in the previous layer, the final layer takes input from the 128 nodes in the layer before it, and outputs a value in the range `[0, 1]`, representing the probability that the image belongs to that class. The sum of all 10 node values is 1.\n",
        "\n",
        "> Note: Using `softmax` activation and `SparseCategoricalCrossentropy()` has issues and which are patched by the `tf.keras` model. A safer approach, in general, is to use a linear output (no activation function) with `SparseCategoricalCrossentropy(from_logits=True)`.\n"
      ],
      "metadata": {
        "id": "PhMtMImfRyG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
        "                           input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])"
      ],
      "metadata": {
        "id": "y-GWSG9hR5d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n",
        "\n",
        "\n",
        "* *Loss function* ‚Äî An algorithm for measuring how far the model's outputs are from the desired output. The goal of training is this measures loss.\n",
        "* *Optimizer* ‚ÄîAn algorithm for adjusting the inner parameters of the model in order to minimize loss.\n",
        "* *Metrics* ‚ÄîUsed to monitor the training and testing steps. The following example uses *accuracy*, the fraction of the images that are correctly classified."
      ],
      "metadata": {
        "id": "we1r5LfzKrB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-oMnQo4RKrvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model\n",
        "\n",
        "First, we define the iteration behavior for the train dataset:\n",
        "1. Repeat forever by specifying `dataset.repeat()` (the `epochs` parameter described below limits how long we perform training).\n",
        "2. The `dataset.shuffle(60000)` randomizes the order so our model cannot learn anything from the order of the examples.\n",
        "3. And `dataset.batch(32)` tells `model.fit` to use batches of 32 images and labels when updating the model variables.\n",
        "\n",
        "Training is performed by calling the `model.fit` method:\n",
        "1. Feed the training data to the model using `train_dataset`.\n",
        "2. The model learns to associate images and labels.\n",
        "3. The `epochs=5` parameter limits training to 5 full iterations of the training dataset, so a total of 5 * 60000 = 300000 examples.\n",
        "\n",
        "(Don't worry about `steps_per_epoch`, the requirement to have this flag will soon be removed.)"
      ],
      "metadata": {
        "id": "kgH0ZWq7K1HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataset = train_dataset.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.cache().batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "zoUsq7TGK5hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate accuracy\n",
        "\n",
        "Next, compare how the model performs on the test dataset. Use all examples we have in the test dataset to assess accuracy."
      ],
      "metadata": {
        "id": "kzmENtnnK_XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset, steps=math.ceil(num_test_examples/32))\n",
        "print('Accuracy on test dataset:', test_accuracy)"
      ],
      "metadata": {
        "id": "IXi_fcd4K7ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(EPOCHS)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.savefig('./foo.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NVX9FAJMSsid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cats and dogs üêï"
      ],
      "metadata": {
        "id": "KuRWI1aMR5sQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model consists of four convolution blocks with a max pool layer in each of them. Then we have a fully connected layer with 512 units, with a `relu` activation function. The model will output class probabilities for two classes ‚Äî dogs and cats ‚Äî using `softmax`. \n",
        "\n",
        "**Compile the model**\n",
        "\n",
        "As usual, we will use the `adam` optimizer. Since we output a softmax categorization, we'll use `sparse_categorical_crossentropy` as the loss function. We would also like to look at training and validation accuracy on each epoch as we train our network, so we are passing in the metrics argument."
      ],
      "metadata": {
        "id": "flY1I6_vR8gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'), # feed that to 512 neuron dense layer\n",
        "    tf.keras.layers.Dense(2)\n",
        "])"
      ],
      "metadata": {
        "id": "iQ4d486dR8xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "4W2-0Sv6SE1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "39VcRMmCSHjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### applying a Dropout\n",
        "Before the final Dense layers, we're also applying a Dropout probability of 0.5. It means that 50% of the values coming into the Dropout layer will be set to zero. This helps to prevent overfitting.\n",
        "\n",
        "Then we have a fully connected layer with 512 units, with a relu activation function. The model will output class probabilities for two classes ‚Äî dogs and cats ‚Äî using softmax. \n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(2)\n",
        "])\n",
        "```"
      ],
      "metadata": {
        "id": "j9tMTmaj0rH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model"
      ],
      "metadata": {
        "id": "FRz97NLUScap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time we train our network.\n",
        "\n",
        "Since our batches are coming from a generator (`ImageDataGenerator`), we'll use `fit_generator` instead of `fit`."
      ],
      "metadata": {
        "id": "7HP_AGj_SgaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(total_val / float(BATCH_SIZE)))\n",
        ")"
      ],
      "metadata": {
        "id": "Mv15xZzBSi_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing results of the training"
      ],
      "metadata": {
        "id": "I8cYCFW3Sowm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pRS38GlOPxm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flowers üå∑"
      ],
      "metadata": {
        "id": "emWJR2FtPfQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    \n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(IMG_SHAPE, IMG_SHAPE, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(5)\n",
        "])"
      ],
      "metadata": {
        "id": "U9OJF2VhQ-Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "R8L_wAjwQ_nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "caWwIT1qRB8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 80\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=int(np.ceil(train_data_gen.n / float(batch_size))),\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(val_data_gen.n / float(batch_size)))\n",
        ")"
      ],
      "metadata": {
        "id": "7Rw9x41wRDjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions ..."
      ],
      "metadata": {
        "id": "7t3CsbUrcWxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clothing "
      ],
      "metadata": {
        "id": "WIxqiSzYLE1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With the model trained, we can use it to make predictions about some images.\n",
        "for test_images, test_labels in test_dataset.take(1):\n",
        "  test_images = test_images.numpy()\n",
        "  test_labels = test_labels.numpy()\n",
        "  predictions = model.predict(test_images)"
      ],
      "metadata": {
        "id": "ZlNlNJXlLSSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.shape"
      ],
      "metadata": {
        "id": "cXxKkBsQLV9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, the model has predicted the probability of each label for each image in the testing set. Let's take a look at the first prediction:\n",
        "predictions[0]"
      ],
      "metadata": {
        "id": "N3lYk-MfLXQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A prediction is an array of 10 numbers. These describe the \"confidence\" of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:\n",
        "np.argmax(predictions[0])"
      ],
      "metadata": {
        "id": "2Zmc7lavLaxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So the model is usually most confident that this image is a Shirt, or class_names[6]. Let's check the label:\n",
        "test_labels[0]"
      ],
      "metadata": {
        "id": "oYZxyfPaLdgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can graph this to look at the full set of 10 class predictions"
      ],
      "metadata": {
        "id": "V7sPheBDLk6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(i, predictions_array, true_labels, images):\n",
        "  predictions_array, true_label, img = predictions_array[i], true_labels[i], images[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  \n",
        "  plt.imshow(img[...,0], cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "  \n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  \n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')"
      ],
      "metadata": {
        "id": "V7ccAsIyLoZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the 0th image, predictions, and prediction array.  \n",
        "i = 0\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions, test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions, test_labels)"
      ],
      "metadata": {
        "id": "xq5Oq7PnLqbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 12\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions, test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions, test_labels)"
      ],
      "metadata": {
        "id": "bCI2rrMPLvem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot several images with their predictions. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percent (out of 100) for the predicted label. Note that it can be wrong even when very confident. "
      ],
      "metadata": {
        "id": "iF8k4JuuLxZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the first X test images, their predicted label, and the true label\n",
        "# Color correct predictions in blue, incorrect predictions in red\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i, predictions, test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i, predictions, test_labels)"
      ],
      "metadata": {
        "id": "fqcyqNe_Ly2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, use the trained model to make a prediction about a single image. \n",
        "\n",
        "# Grab an image from the test dataset\n",
        "img = test_images[0]\n",
        "\n",
        "print(img.shape)"
      ],
      "metadata": {
        "id": "MxjyrfwTL0bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras models are optimized to make predictions on a batch, or collection, of examples at once. So even though we're using a single image, we need to add it to a list:\n",
        "\n",
        "# Add the image to a batch where it's the only member.\n",
        "img = np.array([img])\n",
        "\n",
        "print(img.shape)"
      ],
      "metadata": {
        "id": "EijnacBpL7Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now predict the image:\n",
        "predictions_single = model.predict(img)\n",
        "\n",
        "print(predictions_single)"
      ],
      "metadata": {
        "id": "poOnJe7fL-fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_value_array(0, predictions_single, test_labels)\n",
        "_ = plt.xticks(range(10), class_names, rotation=45)"
      ],
      "metadata": {
        "id": "u5Q5VXOvMCyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.predict returns a list of lists, one for each image in the batch of data. Grab the predictions for our (only) image in the batch:\n",
        "np.argmax(predictions_single[0])"
      ],
      "metadata": {
        "id": "9qCZbPJtMEq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cats and dogs"
      ],
      "metadata": {
        "id": "vJqyZTfxLOGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not sure if code looks good...\n",
        "\n",
        "\n",
        "???????"
      ],
      "metadata": {
        "id": "WLxiI1VEcgKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=model.predict(test_data_gen)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "probabilities=[]\n",
        "for p in predictions:\n",
        "  if p[0]>p[1]:\n",
        "    probabilities.append(0)\n",
        "  else:\n",
        "    probabilities.append(1)\n"
      ],
      "metadata": {
        "id": "QaDjhF02cb70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answers must know, must have before check. "
      ],
      "metadata": {
        "id": "UyZd8dSxcnUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers =  [1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
        "            1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
        "            1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
        "            1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, \n",
        "            0, 0, 0, 0, 0, 0]\n",
        "\n",
        "correct = 0\n",
        "\n",
        "for probability, answer in zip(probabilities, answers):\n",
        "  if round(probability) == answer:\n",
        "    correct +=1\n",
        "\n",
        "percentage_identified = (correct / len(answers))\n",
        "\n",
        "passed_challenge = percentage_identified > 0.63\n",
        "\n",
        "print(f\"Your model correctly identified {round(percentage_identified, 2)}% of the images of cats and dogs.\")\n",
        "\n",
        "if passed_challenge:\n",
        "  print(\"You passed the challenge!\")\n",
        "else:\n",
        "  print(\"You haven't passed yet. Your model should identify at least 63% of the images. Keep trying. You will get it!\")"
      ],
      "metadata": {
        "id": "BmcF5pfPceWT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}